# Byte Pair Encoding (BPE) Tokenizer  

This project is a **minimal implementation of a Byte Pair Encoding (BPE) tokenizer** written in Python, without using existing tokenization libraries like `tiktoken`.  

It can:  
1. **Train** on a text corpus (`alice.txt`).  
2. **Encode** new text into a list of integer token IDs.  
3. **Decode** token IDs back to the original text (lossless for the training set).  

---

## ğŸ“‚ Project Structure 
bpe_tokenizer/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ tokenizer.py # Implementation of BPETokenizer
â”‚ â””â”€â”€ main.py # CLI demo script (train/encode/decode)
â”‚
â”œâ”€â”€ alice.txt # Training corpus (sample from Alice in Wonderland)
â”œâ”€â”€ README.md # Project instructions
â””â”€â”€ output.txt # Example output from running main.py

### 1. Clone the repository  

```bash
git clone https://github.com/<your-username>/bpe_tokenizer.git
cd bpe_tokenizer
```

2. Run the demo

```bash
python src/main.py
```

3. Output
The script will:

Train the tokenizer on alice.txt.

Encode and decode a sample sentence.

Save results in output.txt.

4. Example console output:

```bash
Training tokenizer...

Encoding: Alice was beginning to get very tired
Tokens: [3, 7, 14, 22, 5, ...]
Decoded: Alicewasbeginningtogetverytired

Results written to output.txt
```

ğŸ§ª Example Corpus (alice.txt)
The provided alice.txt contains paragraphs from Aliceâ€™s Adventures in Wonderland by Lewis Carroll, which serves as the training corpus.

ğŸ“ Notes
This is a minimal educational BPE tokenizer, not optimized for production use.

Special tokens, regex pre-tokenization, and Unicode corner cases are ignored for simplicity.

